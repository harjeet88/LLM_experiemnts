{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harjeet88/LLM_experiemnts/blob/main/LLMs_course/week1/ollama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WOz84dHUqk-v"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "\n",
        "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
        "HEADERS = {\"Content-Type\": \"application/json\"}\n",
        "MODEL = \"llama3.2\""
      ],
      "metadata": {
        "id": "7Hbtv75vqvg4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a messages list using the same format that we used for OpenAI\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
        "]\n"
      ],
      "metadata": {
        "id": "r_jHdUU6qzaQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "payload = {\n",
        "        \"model\": MODEL,\n",
        "        \"messages\": messages,\n",
        "        \"stream\": False\n",
        "    }"
      ],
      "metadata": {
        "id": "rjGQbkdRq34d"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -LO https://ollama.ai/install.sh\n",
        "!bash install.sh\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U9VmhaCBtKD",
        "outputId": "a8fcd58d-9269-4b0d-8f06-fb2affe1c861"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13281    0 13281    0     0  62299      0 --:--:-- --:--:-- --:--:-- 62352\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama serve"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CiBBdKIEJOo",
        "outputId": "e88dc292-f6b4-44ee-871a-c4d3ab4def8e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\n",
            "Your new public key is: \n",
            "\n",
            "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICQEwr7LUcngSRf7y9bB0wUziv0/Gy06Q6FhfITs1rWH\n",
            "\n",
            "2025/03/10 07:59:45 routes.go:1215: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
            "time=2025-03-10T07:59:45.732Z level=INFO source=images.go:432 msg=\"total blobs: 0\"\n",
            "time=2025-03-10T07:59:45.732Z level=INFO source=images.go:439 msg=\"total unused blobs removed: 0\"\n",
            "time=2025-03-10T07:59:45.733Z level=INFO source=routes.go:1277 msg=\"Listening on 127.0.0.1:11434 (version 0.5.13)\"\n",
            "time=2025-03-10T07:59:45.733Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
            "time=2025-03-10T07:59:45.770Z level=INFO source=gpu.go:377 msg=\"no compatible GPUs were discovered\"\n",
            "time=2025-03-10T07:59:45.770Z level=INFO source=types.go:130 msg=\"inference compute\" id=0 library=cpu variant=\"\" compute=\"\" driver=0.0 name=\"\" total=\"334.6 GiB\" available=\"329.7 GiB\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's just make sure the model is loaded\n",
        "\n",
        "!ollama pull llama3.2"
      ],
      "metadata": {
        "id": "oYiNQvWaq67_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf02bdbc-2ccd-4bea-ebfe-37d492f81318"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: could not connect to ollama app, is it running?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_documents(document_paths, index, texts):\n",
        "    \"\"\"Ingest documents into the vector database.\"\"\"\n",
        "    for doc_path in document_paths:\n",
        "        try:\n",
        "            with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "                texts.append(text)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {doc_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {doc_path}: {e}\")\n",
        "\n",
        "    embeddings = get_embeddings(texts[-len(document_paths):]) #generate only the new embeddings.\n",
        "    index.add(embeddings)\n",
        "    print(\"Documents ingested successfully.\")\n"
      ],
      "metadata": {
        "id": "5Nhamjclq9gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_context(question, index, texts):\n",
        "    \"\"\"Retrieve relevant context from the vector database.\"\"\"\n",
        "    question_embedding = get_embeddings([question])\n",
        "    D, I = index.search(question_embedding, 5) # Search top 5\n",
        "    relevant_texts = [texts[i] for i in I[0]]\n",
        "    return \" \".join(relevant_texts)"
      ],
      "metadata": {
        "id": "PdmtzqiKrBNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(context, question):\n",
        "    \"\"\"Generate an answer using the Flan-T5 model.\"\"\"\n",
        "    input_text = f\"context: {context} question: {question}\"\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "    outputs = model.generate(input_ids, max_length=50)\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "H0CNGnh5rD5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_pipeline(question, index, texts):\n",
        "    \"\"\"RAG pipeline using FAISS and Flan-T5.\"\"\"\n",
        "    context = retrieve_relevant_context(question, index, texts)\n",
        "    if context:\n",
        "        answer = generate_answer(context, question)\n",
        "        return answer\n",
        "    else:\n",
        "        return \"No relevant information found.\"\n"
      ],
      "metadata": {
        "id": "-F0nS9NNrK1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "texts = [] # List to store document texts\n",
        "embeddings_dim = 768  # Dimension of embeddings from all-mpnet-base-v2\n",
        "index = faiss.IndexFlatL2(embeddings_dim) # Create FAISS index\n"
      ],
      "metadata": {
        "id": "BTKsVRxwrLlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create example documents (for testing)\n",
        "#doc1_content = \"The Eiffel Tower is in Paris.\"\n",
        "#doc2_content = \"The capital of Japan is Tokyo.\"\n",
        "#doc3_content = \"Python is a popular programming language.\"\n",
        "\n",
        "# HP Usage\n",
        "\n",
        "doc1_content = \"Harry Potter is a series of seven fantasy novels written by British author J. K. Rowling. The novels chronicle the lives of a young wizard, Harry Potter, and his friends, Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. The main story arc concerns Harry's conflict with Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic, and subjugate all wizards and Muggles (non-magical people).\"\n",
        "doc2_content=\"Harry learns that his parents, Lily and James Potter, also had magical powers and were murdered by the dark wizard Lord Voldemort when Harry was a baby.\"\n",
        "doc3_content=\"He gains the friendship of Ron Weasley, a member of a large but poor wizarding family, and Hermione Granger, a witch of non-magical, or Muggle, parentage.\"\n",
        "doc4_content=\"he first book concludes with Harry's confrontation with Voldemort, who, in his quest to regain a body, yearns to possess the Philosopher's Stone, a substance that bestows everlasting life.\"\n",
        "doc5_content=\"Tom riddle is actual Voldemort. Harry learns from a drunken Slughorn that he used to teach Tom Riddle, and that Voldemort divided his soul into pieces, creating a series of Horcruxes.\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pwQtL-YurOpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"doc1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(doc1_content)\n",
        "with open(\"doc2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(doc2_content)\n",
        "with open(\"doc3.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(doc3_content)\n",
        "with open(\"doc4.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(doc5_content)\n",
        "with open(\"doc5.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(doc5_content)"
      ],
      "metadata": {
        "id": "3ojIwRjZxOrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_paths = [\"doc1.txt\", \"doc2.txt\", \"doc3.txt\"]\n",
        "ingest_documents(document_paths, index, texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw5ZTq1brVl6",
        "outputId": "ff89967c-5109-4f72-e57b-db70e0eb5822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents ingested successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"who are friends of harry potter?\"\n",
        "answer = rag_pipeline(question, index, texts)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gkh2HwtMraNw",
        "outputId": "0534119f-9ad9-4af7-d23d-5300bb685265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: who are friends of harry potter?\n",
            "Answer: Hermione Granger and Ron Weasley\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"who is enemy of harry potter\"\n",
        "answer = rag_pipeline(question, index, texts)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNYKxm9VroUD",
        "outputId": "3acd0730-e1a2-414a-9a74-3bc1be1757fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: who is enemy of harry potter\n",
            "Answer: Lord Voldemort\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BsKxEGKErxuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}