{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQJElw6RlShMwclFvX9BYH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harjeet88/LLM_experiemnts/blob/main/RAG/LLM_RAG_sql_lite_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qNzSuveMupb-"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "import torch\n",
        "import numpy as np #Import numpy\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model and tokenizer\n",
        "#model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "#model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjy1-7fCuvSl",
        "outputId": "9e1bd125-177d-4477-c8a5-0bae0579cf8d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up SQLite connection\n",
        "conn = sqlite3.connect('rag_example.db')\n",
        "cur = conn.cursor()"
      ],
      "metadata": {
        "id": "JFa9KwuT_lKx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create table with pgvector extension\n",
        "cur.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS documents (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    content TEXT,\n",
        "    embedding BLOB\n",
        ");\n",
        "\"\"\")\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "RcY04vOe_o1R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute embeddings\n",
        "def compute_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.pooler_output[0].numpy()"
      ],
      "metadata": {
        "id": "caj4eQyBxli4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inserting documents into the database\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The rain in Spain stays mainly in the plain.\",\n",
        "    \"In a hole in the ground there lived a hobbit.\"\n",
        "]"
      ],
      "metadata": {
        "id": "gWw3es5eu7__"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in documents:\n",
        "    embedding = compute_embedding(doc).tobytes()  # Convert numpy array to bytes\n",
        "    cur.execute(\"INSERT INTO documents (content, embedding) VALUES (?, ?)\", (doc, embedding))\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "NnQhIlRdxQ-K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve documents based on query\n",
        "def retrieve_documents(query, top_k=3):\n",
        "    query_embedding = compute_embedding(query)\n",
        "    cur.execute(\"SELECT content, embedding FROM documents\")\n",
        "    rows = cur.fetchall()\n",
        "\n",
        "    # Compute cosine similarity between query embedding and document embeddings\n",
        "    similarities = []\n",
        "    for content, embedding in rows:\n",
        "        doc_embedding = torch.tensor(list(embedding), dtype=torch.float32).view(1, -1)\n",
        "        similarity = torch.nn.functional.cosine_similarity(torch.tensor(query_embedding), doc_embedding)\n",
        "        similarities.append((content, similarity.item()))\n",
        "\n",
        "    # Sort by similarity and return top_k results\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [content for content, _ in similarities[:top_k]]"
      ],
      "metadata": {
        "id": "aClmqHqmyhHZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve documents based on query\n",
        "def retrieve_documents(query, top_k=3):\n",
        "    query_embedding = compute_embedding(query)\n",
        "    cur.execute(\"SELECT content, embedding FROM documents\")\n",
        "    rows = cur.fetchall()\n",
        "\n",
        "    # Compute cosine similarity between query embedding and document embeddings\n",
        "    similarities = []\n",
        "    for content, embedding in rows:\n",
        "        # Convert the byte embedding back to a numpy array, then a tensor\n",
        "        #doc_embedding = np.frombuffer(embedding, dtype=np.float32)\n",
        "        #doc_embedding = torch.tensor(doc_embedding).view(1, -1) #View is added to ensure correct dimensions\n",
        "        doc_embedding = np.frombuffer(embedding, dtype=np.float32)\n",
        "        doc_embedding = doc_embedding.reshape(1, -1) # Reshape the array to be 1x768\n",
        "        doc_embedding = torch.tensor(doc_embedding)\n",
        "        similarity = torch.nn.functional.cosine_similarity(torch.tensor(query_embedding), doc_embedding)\n",
        "        similarities.append((content, similarity.item()))\n",
        "\n",
        "    # Sort by similarity and return top_k results\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [content for content, _ in similarities[:top_k]]"
      ],
      "metadata": {
        "id": "Ys0AbROwAheF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve documents based on query\n",
        "def retrieve_documents(query, top_k=3):\n",
        "    query_embedding = compute_embedding(query)\n",
        "    cur.execute(\"SELECT content, embedding FROM documents\")\n",
        "    rows = cur.fetchall()\n",
        "\n",
        "    # Compute cosine similarity between query embedding and document embeddings\n",
        "    similarities = []\n",
        "    for content, embedding in rows:\n",
        "        # Convert the byte embedding back to a numpy array, then a tensor\n",
        "        doc_embedding = np.frombuffer(embedding, dtype=np.float32)\n",
        "        doc_embedding = doc_embedding.reshape(1, -1) # Reshape the array to be 1x768\n",
        "        doc_embedding = torch.tensor(doc_embedding)\n",
        "        similarity = torch.nn.functional.cosine_similarity(torch.tensor(query_embedding), doc_embedding)\n",
        "        similarities.append((content, similarity.item()))\n",
        "\n",
        "    # Sort by similarity and return top_k results\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [content for content, _ in similarities[:top_k]]"
      ],
      "metadata": {
        "id": "JTKPQ2n1D1VH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a response\n",
        "def generate_response(query):\n",
        "    retrieved_docs = retrieve_documents(query)\n",
        "    combined_text = \" \".join(retrieved_docs)\n",
        "    generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "    response = generator(f\"Query: {query}\\nDocuments: {combined_text}\\nAnswer:\", max_length=100)\n",
        "    return response[0]['generated_text']"
      ],
      "metadata": {
        "id": "mSehuY86-hgr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query = \"Tell me about foxes and rain.\"\n",
        "response = generate_response(query)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "RTZxZxZX-mGD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "d3bfcc58-ef4b-4633-f321-4df9ae185030"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (768) must match the size of tensor b (1536) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6f3666d8ed57>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Tell me about foxes and rain.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-0166ab51c850>\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Function to generate a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mretrieved_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcombined_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieved_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-5bd77a849e58>\u001b[0m in \u001b[0;36mretrieve_documents\u001b[0;34m(query, top_k)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdoc_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (768) must match the size of tensor b (1536) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Close the database connection\n",
        "cur.close()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "hZB5hnzk-rBV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vRfHfHH3-9T9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}